import utilities as ut






class node:
    amount = 0

    def __init__(self, father, label=None):
        amount += 1
        
        self.father = father
        self.label = label

        self.nth_feature = None
        self.split_val = None

        self.greater_child = None
        self.lower_child = None
        
    
    
    def set_decision(nth, split):
        self.nth_feature = nth
        self.split_val = split


    def set_children(greater, lower):
        self.greater_child = greater
        self.lower_child = lower

    def is_leaf():
        return self.greater_child == None

    def sanity():
        if is_leaf():
            if self.label == None:
                raise Exception("Foglia senza label")

        else:
            if not (self.greater_child != None and self.lower_child != None):
                raise Exception("Node with missing child")
            
            self.greater_child.sanity()
            self.lower_child.sanity()

            


def check_label(data):
    ref = data[0][-1]
    for x in data:
        if (x[-1] != ref):
            return False, None

    return True, ref


def check_feature(data, nth_feature):
    ref = data[0][nth_feature]
    for x in data[1:]:
        if x[nth_feature] != ref:
             return False
    
    return True



X,y = get_data()
data = dataset(X, y)
print(data.entropy)

root = node()


def choose_feature():



def learn(X, y):
    check, label = check_label(data)
    
    if check:
        node = tree(0,0, father=father, label=label)
        return node


    best_info_gain = 0
    best_split = []
    split_val = 0
    index = -1

        for i in range(len(data[0])-1):
        if check_feature(data, i):
            continue

        average = ut.average(data, i)
        data1, data2 = ut.split_average(data, average, i)

        if len(data1) < 1 or len(data2) < 1:
            print(len(data1), len(data2))
            print(data2)
            print(data1)
            raise "GAIA TROIA infame"


        info_gain = entropy_father - (ut.entropy(data1) + ut.entropy(data2))/2
       
        if info_gain > best_info_gain:
            best_split = [data1, data2]
            best_info_gain = info_gain
            split_val = average
            index = i

    
    node = tree(i, split_val, father)
    node.greater_child = data1
    node.lower_child = data2
    print(entropy_father, best_info_gain)

    learn(data1, node)
    learn(data2, node)
    return node




learn(data.X, data.y)
print(root.amount)
